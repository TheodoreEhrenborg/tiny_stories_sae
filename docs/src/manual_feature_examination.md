# Manual feature examination

Look at features 0, 1, 2

all the way to 10?

Proportion of dead features

You can see the raw jsons here TODO

### Feature 1

### Feature 2

### Feature 3

### Feature 4

### Feature 5

## Comparing to raw LLM neurons

My prior is that the LLM activations
won't correspond to specific topics.

But we should give them the benefit of the doubt.

There are three ways we could interpret the activations:

Go through some examples in detail to show that ChatGPT is wrong

### Feature 1

### Feature 2

### Feature 3

### Feature 4

### Feature 5
