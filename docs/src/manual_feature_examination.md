# Manual feature examination

Look at features 0, 1, 2

all the way to 10?



Proportion of dead features

## Comparing to raw LLM neurons
My prior is that the LLM activations 
won't correspond to specific topics.

But we should give them the benefit of the doubt.

There are three ways we could interpret the activations:


Go through some examples in detail to show that ChatGPT is wrong
